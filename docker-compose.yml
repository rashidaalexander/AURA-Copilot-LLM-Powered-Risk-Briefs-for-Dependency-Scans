services:
  backend:
    build: ./backend
    environment:
      - LLM_PROVIDER=ollama
      - OLLAMA_URL=http://ollama:11434
      - OLLAMA_MODEL=llama3.1:8b
      # Optional OpenAI:
      # - LLM_PROVIDER=openai
      # - OPENAI_API_KEY=your_key
      # - OPENAI_MODEL=gpt-4.1-mini
    ports:
      - "8000:8000"
    depends_on:
      - ollama

  frontend:
    image: nginx:alpine
    volumes:
      - ./frontend:/usr/share/nginx/html:ro
    ports:
      - "8080:80"
    depends_on:
      - backend

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama:/root/.ollama
    # The first run will download the model (one-time).
    # After that, it's cached in the 'ollama' volume.
volumes:
  ollama:
